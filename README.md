





Book Genre Classification Model

Stephanie Clark
Western Governors University
Student ID 010411142


September 26, 2025
C964: Computer Science Capstone
Task 2 parts A, B, and D
Part A: Letter of Transmittal	2
Part B: Project Proposal Plan	5
Project Summary	5
Data Summary	6
Implementation	8
Timeline	10
Evaluation Plan	11
Costs	14
Part D: Post-implementation Report	15
Solution Summary	15
Data Summary	16
Machine Learning	17
Validation	19
Visualizations	20
Book Genre Classification - User Guide	23
System Requirements	23
Installation	23
Getting Started	25
Reference Page	28



Part A: Letter of Transmittal 

September 15th, 2025
 
Edgar A. Poe, CTO
Booklytic Inc.
123 Books Rd
Salt Lake City, Utah
 
Dear Mr. Poe, 
In the ever-changing landscape of innovation and technology, Booklytic presents unique opportunities to position itself at the forefront of the literary industry by leveraging machine learning applications that enhance the way data is labeled and handled. By automating the categorization of genres, this application proposal aims to address key challenges faced in the industry, such as data inconsistencies and missing genre labels, which hinder comprehensive data analysis and lead to higher rates of inaccurate recommendations, ultimately impeding user engagement on our platform. By ensuring a seamless user experience, Booklytic can gain a strategic advantage over similarly marketed services.
The current ecosystem of online retail bookstores relies on manual and inconsistent genre classification, which leads to mislabeling and poor data discovery possibilities. As online platforms and their product databases increase in scale, manually identifying genres for books becomes unsustainable. To ensure that the online Booklytic platform can remain viable as its database increases in size, manual genre labeling should be enacted to ensure proper scaling with larger datasets. By smoothing out the process of genre classification and creating a general standard by which genres can be identified, curated systems will be created that can provide users with more meaningful recommendations that correspond with their preferred reading habits, leading to higher user satisfaction with the online interface.
The proposed automatic genre generator applies natural language processing and machine learning to book summaries, ensuring consistency and accuracy at a large scale. This type of classification model will lead to improved search relevance, recommendations, and user satisfaction. On a business level, this leads to smarter, high-level decisions surrounding curation, marketing, and user engagement at Booklytic. By making the data within the database reliable and actionable, these lead to better business decisions based on that data.
The primary deliverables will be a genre classification model that analyzes book summaries and assigns genre tags accordingly. The surrounding application will include a user input section for users to submit their own summary entry for genre labeling, a cleaned and normalized dataset with pre-determined genre categories, a clean and easy-to-use UI, and a dashboard with various graphs visualizing the model’s data and accuracy, such as a pie chart that reviews genre counts, a line graph with training and validation loss information, and a heatmap that examines pre-labelled genres vs the model’s predictions.
The model will be trained on a large dataset of summaries and pre-labelled genres, and a supervised model called DistilBERT will be used. The database will be obtained from the “CMU Book Summary Dataset” from Kaggle. This provides over 16,000 examples of books, their summary, and their associated genre pulled from Wikipedia articles. This provides a significant and public source of various types of books for the model to analyze.
The objective of this application is to create a reliable and scalable system that can predict genres with 70% accuracy against the validation dataset. This will, in turn, enable Booklytic to reduce manual labeling efforts and improve user engagement within the platform. A supervised NLP model can accurately enhance the genre labeling process at Booklytic and give accurate genre classification abilities with minimal human involvement, which will lead to improved searchability on the Booklytic website.
To create an application that predicts a book’s genre based on its summary, the project’s approach will be based on the CRISP-DM methodology throughout all planning, development, and documentation stages. This will ensure that all aspects of development are handled in an orderly and universal fashion. Development will proceed as follows…
Firstly, conversations between Booklytic and the development team will take place to ensure all required business and technical needs are met before development. This will include defining the project’s scope, final model selection, UI development requirements, and deployment within Booklytic’s systems. Next, data will be reviewed, and target genres will be defined based on genre availability within the dataset. Initial exploration will be done to understand the structure and quality of the data, purposefully verifying any potential genre biases. Before model training, data will be prepared for training by cleaning and tokenizing the dataset. The primary dataset will be divided into an 80/20 split for training and validation testing. When the dataset is ready for modelling, a pretrained transformer model will be utilized to carry out this categorization task. The model will be trained on the reviewed and cleaned dataset, validating the results against the 70% accuracy metric previously noted. The model will be evaluated by a general 70% accuracy metric, with further verification provided by F1, precision, and recall scores located within the application. A user-friendly UI will be developed that allows users to input book summaries, and the top 3 most likely genre labels will be output to the user interface. The final system will integrate front and back-end components, including a user manual for easy access to the application functionality.
An estimated budget of $13,350 is requested to cover the development and deployment of the genre classification product. This will cover the employment of a Lead Developer for 50 hours at $55/hr, a data scientist for 46 hours at $60/hr, and a QA Tester for 16 hours at $35/hr. Two computers will need to be available, costing about $5,000, to ensure that the systems are robust enough to run the model in a reasonable amount of time for testing. Additional hardware and software expenses include four monitors ($1,200 total), two PyCharm Professional Edition licenses ($400), and two Windows 11 Pro licenses ($280), bringing the total hardware and software cost to $6,680. The dataset will be initially obtained through Kaggle, an online dataset proprietor, for free.
This model will bring many benefits to all stakeholders of Booklytic by automating labeling tasks and allowing for more of the Booklytic team’s focus to be on users and their experience with Booklytic. Readers will receive more accurate book recommendations and more comprehensive search results on the platform; publishers will gain increased visibility for niche genres; UX developers will have a streamlined tool for enhanced data management; and advertisers will gain significant advantages through increased opportunities for targeted genre advertising.
Ethical data practices will be strictly enforced and maintained to ensure that all Booklytic users and their data are safe from unauthorized access to proprietary and private information. The model will use no personally identifiable information (PII), only the summaries and the book’s associated genre. Transparency of the model and dataset will be a main priority for the development team, along with ensuring that biases within the predictive model are identified and avoided.
As the developer of the application, I bring a background in NLP models, machine learning, and experience in building scalable AI products that specialize in classification. My expertise will ensure that this machine learning solution is technically robust and valuable to all of Booklytic’s stakeholders.
Sincerely,
Stephanie Clark
Stephanie Clark, Book Data Scientist & Developer

Part B: Project Proposal Plan
Project Summary
Genre labeling for online retailers can be a tedious task that is reliant on manual labor. This can lead to inconsistent search results and search engines that lack depth for providing unique and relevant information, specifically to Booklytic users. By automating the labeling process of books, the additional labor needed compared to manual entry is significantly reduced, while also providing a consistent and powerful tool. The creation and implementation of this tool within Booklytic’s software architecture will lead to better business outcomes and a better user experience with their online platform.
Our client, Booklytic, is an online book retailer that utilizes user-provided reading behaviors to deliver personalized book recommendations. Recently spurred by negative user feedback, Booklytic’s domain specialists verified that recommendations within the application were reported as inaccurate due to inconsistencies in genre labeling throughout Booklytic’s database, which limited the effectiveness of the recommendation services. To address this issue, a machine learning application will be created and integrated within Booklytic’s system. This will enhance the metadata quality of each book, improving the system’s recommendation capabilities.
The following deliverables will be designed to ensure full functionality, usability, and transparency of the machine learning model and the supporting application. These will include: (1) a cleaned and preprocessed dataset, (2) a trained machine learning model that can make predictions with 70% accuracy, and (3) a user-friendly interface that will be accessible to both technical and non-technical users. To support model interpretability and gain user trust, the application will include a dashboard that will consist of key performance indicators, such as a classification report (including precision, recall, and F1 scores), a confusion matrix heatmap across genres, a line graph showing training and validation loss over time, and a pie chart showing the distribution of genres across the dataset. Finally, to ensure ease-of-use, a comprehensive user guide will be provided. Outlines of each system component will be identified and explained, specifically including instructions for dataset input and a prediction workflow within the user guide.
This project aims to deliver a scalable, automated machine learning solution that generates genres based on book summaries. To address current shortcomings within Booklytic’s search and recommendation systems, such as inaccurate metadata, this solution will enable the company to improve content discoverability and user satisfaction, while also reducing labor costs associated with manual labeling. The integration of this application will enhance the quality of data being fed to recommendation algorithms while also creating a consistent labeling framework that can be implemented across all Booklytic products. These steps towards modernizing Booklytic’s data infrastructure will strengthen Booklytic’s competitive position in the online book retail market.

Data Summary
The primary data source for this project will be the publicly available dataset “CMU Book Summary Dataset,” accessed through the Kaggle online data platform. This comprehensive dataset is made of over 16,000 book entries, each containing key book metadata, such as book titles, authors, genres, and summaries (Ymaricar, 2018). Originally scraped from Wikipedia articles by researchers at Carnegie Mellon University, this dataset provides a comprehensive pool of information required to accurately and effectively construct a genre prediction machine learning model.
To prepare the dataset for classification modeling, specific preprocessing steps will be taken before training to ensure optimal model performance. This will include converting all text to lowercase, removing punctuation, and normalizing the data. The final result will be a cleaned, tokenized dataset ready for training. This preprocessing pipeline will ensure accurate model training and establish a standard for data processing after model implementation.
During the development phase, the raw dataset pulled from Kaggle will be transformed into a format readable by the model, such as the CSV format. The data will undergo preprocessing, as previously indicated, specifically relating to handling missing values, normalizing text, and standardizing the genres to be used within the model, as well as tokenization and embedding generation that integrate properly with the model’s input layer. Following this preprocessing, the dataset will be split into training and validation subsets with an 80/20 split. The training subset will be used to optimize the model. In contrast, the validation subset will be saved to verify the model's performance against unknown book summaries and to validate against overfitting. Once this pipeline is finalized, the processed dataset will be fed into the model for training. This finalized trained model will be automatically stored within the desktop application’s directory, ensuring ease of integration with downstream components.
During the deployment phase, the desktop application will be transitioned from training to analyzing real-time predictions. It is expected that when a user submits their own summary into the application, the system will insert the summary into the created preprocessing pipeline and allow for the data to be cleaned in the same way that the original dataset was cleaned for model training. This will ensure that the data format is consistent with the model’s input layer requirements. After the user’s summary has been run through the preprocessing pipeline, the data will be passed through the trained model, returning the probability of each genre individually. The application will take the top 3 highest predicted genres and output them to the application. Additionally, a logging and monitoring framework will be implemented within a CSV file, including preprocessing metadata and prediction scores. These logs will support future auditing and error tracing, enabling model performance monitoring post-application implementation.
In the post-deployment phase, ongoing maintenance will be done to ensure the system remains accurate, relevant, and compliant. Occasional dataset refreshes will be generated to account for inconsistencies and outliers identified within the logged user inputs and predictions, reflecting evolving user needs and emerging trends. Each dataset refresh will result in a required retraining of the model, the process of which will be included within the user guide. Performance metrics will be systematically evaluated to confirm the proper training of the model and confirm that the retraining results improve and expand the results of previous models. The periodic audits of data preprocessing pipelines, access controls, and application storage practices will adhere to CCPA (California Consumer Privacy Act) and GDPR (General Data Protection Regulation) requirements, ensuring that all applicable local, national, and international regulations are followed and adhered to (California Department of Justice; GDPR). Routine backups of the model and user-input logs will be implemented to ensure that the application remains resilient and that business continuity is maintained in the event of system failures.
	The “CMU Book Summary Dataset” encompasses a broad and diverse collection of information that spans numerous literary genres, providing a solid foundation for model development. Before training, an overarching review of the dataset will be done to analyze its quality and genre balance, ensuring that no genre is underrepresented, which would otherwise impair the model’s ability to analyze and categorize. If left unaddressed, class imbalances can lead to model bias and overfitting. To mitigate these types of risks, a pie chart representing the frequencies of each genre will be generated and reviewed before final implementation of the model. In addition to genre review, the dataset will be examined to ensure that no key fields, such as summaries or genres, are missing from within the dataset. This step in validation ensures that the dataset meets the model’s input requirements, minimizing the likelihood of inconsistencies in model predictions. These preprocessing steps will ensure that the final training dataset is robust and reliable, thereby reducing the possibility of training errors and enhancing the model's overall performance.
The “CMU Book Summary Dataset” does not present any known ethical or legal concerns, as the dataset contains no PII (personally identifiable information), sensitive user data, or proprietary information. The data included was scraped from Wikipedia and uploaded to Kaggle, which provides information specifically for public use and research.

Implementation
Development will follow the CRISP-DM (Cross-Industry Standard Process for Data Mining) methodology, summed up by the words of the Data Science Process Alliance, “any good project starts with a deep understanding of the customer’s needs.” This well-established framework will guide the project through six main structured phases: Business Understanding, Data Understanding, Data Preparation, Modeling, Evaluation, and Deployment. This approach to development enables the development team to maintain the ability to fluidly make decisions amid an ever-changing industry, emphasizing both technical finesse and business goals. Development will remain transparent and adaptable, supporting effective communication to all stakeholders. This methodology will serve as the primary foundation for development, ensuring that all components integrate seamlessly into a clear and straightforward pipeline.
	The initial phases of development will focus on thoroughly understanding Booklytic’s business needs and objectives, ensuring that the development takes extra care to consider how this implementation will impact business operations. The key challenge being addressed by this application is the inaccuracy of book recommendations within the Booklytic database, stemming from inconsistent manual genre labeling. These discrepancies within the database lead not only to inferior recommendation results, but the manual labeling process also requires substantial manual labor, increasing operational costs. To ensure that collaboration with stakeholders is prioritized, interviews and requirement-gathering sessions will be conducted to clearly define the project’s business objectives, establish measurable success criteria, and identify any potential technical or operational constraints. The general goal will be to minimize required manual data labeling while improving genre labeling quality to enhance the current book recommendation capabilities. Meeting these goals will enhance user satisfaction while ensuring that Booklytic can strategically place itself in a position to have more scalability and cost-efficiency.
	The Data Understanding phase of development will focus on obtaining and reviewing the dataset to create a solid base for modelling. Data collection will be done using the “CMU Book Summary Dataset” from Kaggle, ensuring that a sufficient amount of data is provided that meets project objectives. The following activities will be prioritized during this phase: data exploration to assess genre distributions and identify data outliers, evaluation of data completeness and consistency, and planning for preprocessing pipelines and tokenization. All of these activities will be crucial for improving the accuracy and robustness of the trained model. Thoroughly understanding the dataset will reduce repetitive work and ensure alignment with both business and technical requirements.
	The Data Preparation phase will transform the raw dataset into a format that can be fed into the input layer of the model. This is important for ensuring that the dataset is being completely and accurately loaded into the model for training. Key tasks during this phase will include data cleaning to address inconsistencies within the data, establishing the preprocessing and tokenization pipeline, encoding each genre label, and partitioning the dataset into an 80/20 split between training and validation. By implementing this workflow, this phase ensures that data is uniform and reliable, creating a more stable and accurate genre prediction model.
	During the Modeling phase, the preprocessed dataset will be used to develop a multi-label genre classification model. The configuration of DistilBERT will be done to accommodate the preprocessed dataset, ensuring that an appropriate loss function for a multi-labelled output is implemented. Next, the training and fine-tuning of the model will be done, with a gradient-based optimization algorithm from AdamW being used to minimize the loss function and prevent overfitting. These steps will ensure that the model is capable of accurately predicting genres based on provided summaries while being able to generalize well against information it has not been trained on.
	The Evaluation phase will consist of reviewing the model’s performance against key metrics, such as precision, recall, F1-scores, and validation loss per epoch. Reviewing these metrics will ensure that technical and business objectives are met, combining quantitative metrics and visual diagnostics to verify model accuracy and usability. A performance assessment will be conducted against the current business database to ensure that the genre spread of the model aligns with Booklytic’s database’s current genre distribution, thereby ensuring that business needs are being met. Quantitative metrics to be reviewed include precision, recall, F1-score, validation loss, and macro/micro averaging, providing insight into both general model effectiveness and genre-specific performance. Error analysis will be conducted to review which specific genres are prone to misclassifications, such as a confusion matrix heatmap. A line graph will also be generated, showing the average training and validation loss over time during training, which enables the monitoring of potential overfitting. This step will analyze the model for deployment readiness, serving as a sign-off for the final development step and ensuring that the product will deliver value through implementation to Booklytic’s current recommendation system.
	In the Deployment phase, the validated model will be finalized and integrated into a user-friendly interface that will be incorporated into the Booklytic systems. The deployment will ensure that users can input a summary and receive a prediction with a confidence score to convey model certainty. Application integration will be done by embedding the trained model into a desktop application and using model APIs to allow for real-time inference. The application will be developed to ensure an intuitive UI that displays genre predictions clearly. A Performance Dashboard will be included to provide stakeholders with insight into the model’s performance, including a genre confusion matrix, accuracy metrics, and a graph of loss over time. All application functions will be thoroughly documented in the user guide for ease of use, covering system requirements, installation procedures, and maintenance information. Validation testing will be conducted to ensure that all components within the application are functioning as intended and to resolve any integration issues, thereby ensuring reliability. PyInstaller will be utilized to deploy the application into an .exe file for ease of use for users. These steps will ensure that the application is fully functional and meets all technical and business metrics, enabling Booklytic to fully utilize this genre model to enhance book recommendations and reduce manual labor costs.


Timeline
Milestone or deliverable
Project Dependencies
Resources
Start and End Date
Duration
Finalize the project scope, deliverables, methodology, and resource plan. Stakeholder approval needed
N/A
N/A
Sept 30 - Oct 1
10 hours
Complete data gathering and review the dataset to ensure data integrity.
Finalize project scope.
“CMU Book Summary Dataset” from Kaggle, and two computers are ready.
Oct 2
6 hours
Identify missing values within the dataset and standardize the data. Document how the data was preprocessed.
Data gathering.
PostgreSQL
Oct 3
8 hours
Select model architecture, train the model on data, implement data analysis classes, and evaluate performance.
The dataset is cleaned and standardized.
Pycharm
Oct 4 - 10
40 hours
Define UI requirements based on the original discovery information. Develop front-end components and gain final approval from clients.
Stakeholder approval of the project.
Pycharm
Oct 11 - 14
16 hours
Integrate UI with back-end, run usability testing, and debug any UI issues.
Both the model and UI need to be completed.
Pycharm
Oct 15 - 17
20 hours
Deploy the solution to production, conduct acceptance testing for the UI, create a user guide, and obtain final sign-off from stakeholders.
Testing must be conducted within acceptable parameters.
PyInstaller
Oct 18 - 21
12 hours


Evaluation Plan
	During the Data Understanding and Data Preparation phases of development, the obtained dataset will be analyzed by the development team to identify any inconsistencies within the data that could affect model training and validation results. The original dataset will utilize a class that parses and cleans inputs before training and within the application to parse user inputs. Functions will include detecting missing data and removal before training, normalization of genres, removal of stopwords, and removal of punctuation. This will ensure that high-quality data is being fed into the model and any user inputs are structured appropriately towards the intended genre modelling task. To further ensure that genre imbalances are reviewed and acknowledged, a chart showing the genre count within the provided dataset will be created, removing any genre that has fewer than 100 examples provided to lessen the impact of class imbalances (this will be done after genre labels are normalized). During this phase, the dataset will be tokenized using the DistilBERT tokenizer, appropriately padding and truncating based on input constraints of the model. These evaluations will ensure the data is of high quality and that deliverables remain relevant to stakeholders.
	Following the parsing and normalization of data, unit testing will be implemented to verify the preprocessing pipeline, ensuring that data is correctly formatted for model training, and is vital to ascertaining model accuracy. By providing data integrity at this stage, it lessens the possibility of errors arising from data irregularities, making it easier for developers to fix bugs and ensure that the project remains helpful to stakeholders.
	Following model training, the model will be validated against the validation subset to assess its accuracy against unknown summaries. The following metrics will be analyzed for accuracy: F1-score, recall, genre confusion matrix, and training and validation loss over time. These will provide high-level views of general model performance while also providing genre-specific results that can be used to fine-tune less accurate genres. Overall, the model’s performance will be viewed as acceptable if overall accuracy meets or surpasses 70% against the validation data subset. This metric will ensure that the model can make meaningful predictions and that the project remains valuable to stakeholders.
	Following model validation and UI integration, a user testing phase will commence to verify the system’s usability and functionality. This phase will guarantee that stakeholders receive the intended value from the final product. Testing will be conducted to confirm that all functional UI is working as intended, reviewing items such as buttons, input fields, and dropdown menus. It will be verified here that user input is being fed through the same preprocessing pipeline that the original dataset was parsed through. Users will be asked explicitly about the usability and intuitiveness of menus, button prompts, and feedback messages. User walkthroughs will be done to identify any usability bottlenecks that could hinder users. Input validation and user handling will be conducted to ensure that the application functions as intended with unintended inputs. Any additional user comments will be taken to management and development to identify and fix any potential issues not previously foreseen. These actions will ensure that the application is user-friendly and that the model makes valuable predictions.
	Additional validation efforts will take place against the generated visualizations by the application. Validations on the visualizations will be done to ensure that the model is accurately monitored and analyzed, leading to better outcomes throughout the entirety of the development pipeline. The following visuals are intended for model review - loss graph for training and validation, genre confusion matrix, chart showing genre distributions, and a classification report (includes accuracy, precision, F1-score, and model averages). These visualizations will be monitored throughout development and will be provided to users after implementation for model transparency and a deeper understanding of the model’s limitations. These will contribute to a continuous validation loop that will occur throughout all stages of development, aligning well with the iterative style of the CRISP-DM methodology.
	To wholly verify the functionality of the application, an adaptable validation strategy will be used that assesses the genre classification tool’s performance, reliability, and usability. This will ensure that predictions remain reliable and intuitive. A technical evaluation will take place that validates the predictive performance of the model, splitting the dataset 80/20 into a training subset and a validation subset and validating the model’s capabilities against unseen summaries. After the model has been trained against the training subset of data, the validation subset will be used to verify how the model reacts to unknown data. The threshold for acceptability will be a minimum of 70% accuracy against the validation subset. If this is not met, the genre confusion matrix will be reviewed to verify any genre biases. Beyond technical validation, user validation testing will be done to ensure that the application is easy to use and helpful to stakeholders. Functional testing will be done to ensure that all UI components are interactive and usable, functioning as intended under normal or unexpected conditions. The reviews will generate confidence in the application's abilities and provide clear indicators about the model’s efficiency and usability.
	To promote transparency and support user understanding of the model, the model will include the top-3 genre predictions with each summary, each accompanied by a corresponding confidence score provided by the model. This enables users to interpret their results more effectively and provides them with tools to analyze their own results more deeply. By displaying not just the highest predicted genre, this classification model weaves the line of ambiguity the model walks when genres' labeling overlaps. This inclusion of prediction confidence scores will enhance prediction interpretability, supporting high-level human review of prediction scores and increasing user confidence in model predictions. This visualization within the application enhances user interpretability while adhering to ethical deployment standards.
	Following the conclusion of model training, a confusion matrix heatmap will be generated automatically with the results from validation testing and made accessible through the application’s dashboard, allowing for a detailed view of accuracy across each genre. Each cell will indicate the confusion intensity, both numerically and through cell shading, providing developers and users with visibility into how each genre compares to the validation set. A confusion matrix will indicate to developers which genres may need fine-tuning to account for biases within the dataset, enabling targeted adjustments to the model architecture, training, or preprocessing pipeline.
	Following internal testing by the development team, user validation testing will take place, verifying the accuracy and usability of the genre classification tool from an end-user perspective. User testing is a vital phase that puts the tool into a real-life scenario to ensure that the application can be used as intended. During this phase, the application will be deployed to specified users, a guide will be created for these users to follow in order, and all users will be given a set of summaries to input, noting any hindrances along the way. The user will then be asked to qualitatively analyze their experience and review whether they received contextually meaningful predictions. A questionnaire including any noteworthy comments and rating for the following topics: User Interface Design, Ease of Use, and general usefulness within Booklytic’s business architecture. This development team sees manual user testing as an essential step to keeping stakeholders and users involved and giving ample opportunities for end-users to provide suggestions that could enhance the clarity and confidence of the model. Some of these final touches to the model will ensure that the final product produces a seamless and user-centered experience.
From a business perspective, the genre prediction tool will be reviewed against Booklytic’s post-implementation database labels. A baseline snapshot of genre and summary metadata will be copied from Booklytic’s database. The snapshot will be manually analyzed for inconsistencies, and a percentage of inaccuracy will be calculated. After the implementation of the genre labeling tool, that same set of summaries will have their genres assigned by feeding their summaries into the genre prediction tool, with an inaccuracy percentage being created that is comparable to the snapshot’s inaccuracy score. These percentage scores will be defined through a human validation agreement, where human reviewers compare tags from both systems and rate each set based on clarity, consistency, and perceived accuracy. This will give stakeholders tangible evidence of the improvements to the system through the use of this automatic genre classification tool.
During the post-deployment period, the system’s performance will be reviewed monthly (for up to 6 months) to ensure that the genre classification tool remains sound and stable, with iterative improvements anticipated. Key performance metrics will be reviewed from the provided dashboard, ensuring that the model is continuing to be relevant and accurate. User input logging will be examined to identify potential improvements or fine-tuning that can enhance overall performance, with iterative enhancements expected throughout the classification tool's lifespan.

Costs 
	Total estimated costs of the project, including hardware and software acquisition, personnel costs, and post-deployment product support. For hardware, two high-performance desktop workstations with NVIDIA RTX 4080 GPUs are needed for model training tasks, equating to a combined cost of ~$5,000. To support model development and testing tasks, two pairs of monitors will be acquired at a combined price of $1,200. Software requirements include two PyCharm Professional Edition (version 2024.3.5) licenses at $200 each, and two Windows 11 Pro licenses at $140 each, totaling $280. In total, hardware and software have an estimated cost of $6,680.
	Labor costs are predicted based on predicted task durations and role-specific hourly rates. The lead developer, responsible for application logic, integration, and implementation, is expected to work approximately 50 hours at a rate of $55/hour, totaling $2,750. A data scientist will be needed to focus on data preprocessing and model fine-tuning, contributing an estimated 46 hours at a rate of $60/hour, totaling $2,760. Finally, a QA tester will be hired for 16 hours at a rate of $35/hour, totaling $560. Between all personnel costs, the total is $6,070.
	Post-deployment maintenance has been budgeted at $200 each month for two months following release, totaling $400. This includes data monitoring, bug identification and fixes, and verification of application stability. Ensuring ongoing maintenance will ensure that the application remains scalable and practical.
	In total, it is expected that this project will total $13,150 between hardware, software, labor, and maintenance costs. These investments are essential to ensuring the accuracy, usability, and accessibility of the model. The hardware and software costs are crucial in providing adequate computational infrastructure that is required to create a meaningful genre prediction machine learning model. Personnel costs ensure that a lead developer, data scientist, and QA professional can be gathered who, combined, ensure that this product remains scalable and maintainable long-term. Finally, the post-deployment budget will ensure that the genre classification model can remain relevant continuously throughout the product's lifetime. All combined, this budget will support the implementation of a high-quality genre classification tool that meets business and user needs.



Part D: Post-implementation Report
Solution Summary
Online book retailers often face the manual labor-intensive task of individually labeling books within their databases, either through in-house cataloging or by relying on user volunteers. This process leads to increased inefficiencies and inconsistencies within their processes and procedures, resulting in inaccurate categorization of genres that negatively affect the search functionality, user recommendations, and customer satisfaction with the model. These inaccuracies ultimately diminish the value that readers, publishers, and retailers receive from the platform, impacting decisions and revenue.
To address these challenges, a supervised machine learning model was developed and fine-tuned to predict a book’s genre based on its summary. By utilizing the DistilBERT model, English linguistic patterns were analyzed and transformed into corresponding genres, enabling accurate genre predictions when processing book summaries previously unseen by the model. This automated classification approach reduced the need for manual labor, improved data quality, and enhanced the reliability of recommendations on the platform, ensuring that stakeholders derived significant value from the service.
The Book Genre Classification Application directly addresses Booklytic’s core issues with inconsistent manual genre labeling while reducing manual labor costs by automating the classification process with a supervised machine learning model. Through the application of the CRISP-DM methodology, the system was developed in structured phases that ensured business alignment throughout all stages of development. Once trained, the model was able to create accurate genre predictions over 70% of the validation set during development, significantly reducing the need for manual intervention. This approach has improved general quality within the Booklytic architecture, strengthening the reliability of Booklytic’s current recommendation system.

Data Summary
The primary data source for this project was the publicly available “CMU Book Summary Dataset,” accessed through the Kaggle online data platform. This comprehensive dataset is made with over 16,000 book entries, each containing key metadata, including titles, authors, genres, and summaries. Originally scraped from Wikipedia articles by researchers at Carnegie Mellon University, the dataset provided a robust pool of information that was used to accurately and effectively construct a genre prediction machine learning model.
Effective data management was pivotal to the development of the genre labeling model, with each step of design, development, deployment, and maintenance phases being propelled by the CRISP-DM methodology. Within this methodology and framework, data was the center of all decision-making, ensuring that all decisions within the data pipeline were grounded in predefined standards and reproducible processes.
During the design phase, quality data management was the foundation that ensured business objectives were upheld in relation to data availability and quality. Reviews of genre distributions, missing summaries, and inferior metadata were heavily assessed against the model input layer's requirements, ensuring that the amount of data was substantial enough for unbiased model training.
During the development phase, it was important for the development team to keep standardization and reproducibility in mind. Preprocessing pipelines were created to clean, tokenize, normalize, and embed data, ensuring consistency across all experiments on the model. The dataset was split during this phase into an 80/20 split between the training and validation subsets, used to review the model’s generalization abilities against overfitting.
During the deployment phase, data management was reviewed against real-time usage by users. The iterative nature of development resulted in a polished and robust preprocessing pipeline, which facilitated a more straightforward implementation in a live environment, as the system is more robust against unique user inputs. The consistency that this preprocessing pipeline gives guarantees that standardized inputs are fed into the model, improving the reliability of the model. To ensure that outliers can be identified more easily by the development team, a logging system was implemented to create a record that enables stakeholders to review outcomes of the model in relation to specific types of data conditions.
During the maintenance phase, data was continually being evaluated to ensure that the model was performing as intended, with periodic updates to ensure that emerging pop-culture trends and evolving user behaviors were analyzed and integrated with the model. Data cleaning and formatting efforts were modularized to keep the application code simple and easy to follow, with comprehensive commenting included. This ensures that the preprocessing pipeline can adapt to new datasets seamlessly, maintaining consistency throughout the application's lifetime. Routine audits were done to validate the continuing accuracy and effectiveness of the model against earlier iterations. This iterative and well-documented approach to development made certain that the genre prediction system remained accurate, scalable, and adaptable, laying the groundwork for future improvements to the Booklytic architecture.
Machine Learning
A supervised machine-learning model, DistilBERT, developed by Hugging Face, was used to create a genre classification tool. DistilBERT, derived from the original BERT model, was designed to excel at understanding language while reducing computational resources, such as memory and processing time, allowing the model and application to run on environments with limited resources. 
The model was configured to accept a summary as input and can produce predictions with over 70% general accuracy against the validation subset of the original dataset. For this application, the genres within the dataset include Sci-Fi, Romance, Fantasy, Horror, Adventure, Historical Fiction, Mystery, Nonfiction, and Children's /YA. The DistilBERT architecture can analyze linguistic patterns from book summaries and create associations to corresponding genres. The application outputs the top 3 predicted genres with a corresponding confidence score of the model. This allows users to analyze their results and gives more clarity about the abilities of the model. By developing a model that automates the manual task of genre classification, Booklytic was able to reduce human effort and minimize inconsistencies that were reducing the accuracy of platform prediction recommendations.
Since DistilBERT is a pre-trained transformer-based model, the development team did not have to train the model from scratch on fundamental linguistic structures. Instead, the focus was on refining the model for the specific task of multi-label genre classification using a preprocessed dataset for supervised learning. Before model training, the dataset underwent cleaning and normalization in the form of removing stopwords, lowercasing summaries, and removing punctuation. These preprocessed summaries were then passed through the DistilBERT tokenizer, formatting the dataset into contextual embeddings for input into the model. Each distinct genre was encoded into a multi-hot vector representation, which allows the model to assign multiple genres to one summary.
The fine-tuning process utilized the BCEWithLogitsLoss function, which was specifically chosen because it is well-suited for use with multi-label classification tasks due to its ability to apply a sigmoid and binary cross-entropy on multiple outputs simultaneously. Training progressed through epochs, where forward propagation generated the predictions to train on, and backpropagation (while being optimized using the AdamW algorithm) updated the model’s parameters, at the same time minimizing the loss function while reducing overfitting.
To decrease overfitting, a checkpoint mechanism was integrated within the application. The model was tested against the validation dataset, with only the most accurately trained model being retained for the application. Throughout all of model testing, performance metrics, such as accuracy, precision, recall, F1-score, and loss values, were all logged and visualized within the application's dashboard. This allowed developers to gain a transparent view of the model’s training progress in real-time.
Finally, once the model was stable and consistently met accuracy metrics across multiple runs, the application was packaged, saved to GitHub, and prepared for final deployment. The deployment process included saving the final model weights and ensuring all visualizations could be accessed within the application dashboard.
DistilBERT was selected as the base of this genre classification model because it offers a balance between efficient computing and accurate genre predictions, allowing for reductions in training and inference time, along with memory usage, making the model advantageous when creating a machine-learning application that can be used on any type of modern device. Through the leveraging of DistilBERT, the application can deliver fast responses with lower hardware requirements, increasing accessibility to users while maintaining accuracy.

Validation 
	The model used, DistilBERT, is classified as a supervised machine learning model. This supervised learning algorithm was able to take the preprocessed dataset, which included summaries and genre labels, and learned the explicit mappings between the summaries and genres. DistilBERT was calibrated to perform multi-label classification, meaning it can assign multiple genres to a single summary, which accounts for books that span several genres and ensures accurate and nuanced classifications. By enabling the model to learn contextual relationships between words in the summaries against their associated genre, the model can generalize against unknown summaries and maintain accuracy.
	To assess model performance, the original dataset was split into training and validation subsets using an 80/20 split. The training set was used to fine-tune DistilBERT’s parameters, while the validation subset was used to verify the model’s ability to generalize against unknown summaries, reducing the likelihood of overfitting. During evaluation, precision, recall, F1-scores, and training/validation loss were computed and reviewed to measure both overall model and specific genre performance. Additionally, a confusion matrix heatmap was created and implemented within the application dashboard, allowing developers to review specific genres for improvement. Visualizations of training and validation loss over time were created to review overfitting and convergence tendencies, allowing for more transparency and value to stakeholders. These performance validation methods ensure the technical and business objectives of the project were met, showing model robustness and generalization capabilities.
	The model’s performance was evaluated against the validation dataset, which was split from the original dataset before training to ensure that an unbiased assessment of the model’s generalizing capabilities could be reviewed. The main objective was to achieve 70% accuracy across the validation dataset, ensuring that the model was able to respond well to unknown summary inputs. Validation loss per epoch was monitored during training to track convergence and overfitting, with a visualization being included within the dashboard. Together, these evaluation measures ensure that the model is robust and accurate.
The model was evaluated against the validation dataset with accuracy, precision, recall, and F1-score for added transparency. The results indicated that the model achieved an average precision of 0.73, a recall of 0.68, and an F1-score of 0.68, showing good generalized accuracy against the validation set. Validation loss per epoch consistently decreases until it plateaus, indicating convergence with minimal overfitting. This confirms that the model meets the initial precision target of 70% accuracy. A visualization showing these statistics is provided on the following page.


Book Genre Classification - User Guide
This guide provides an overview of the DistilBERT-based Book Genre Classification application, which is designed to predict genres from book summaries. This application allows users to submit custom summaries and receive accurate genre predictions. A dashboard is included with visualizations to help monitor model performance.

System Requirements
Windows 11
8 GB of RAM

Installation
Option 1: Use the Executable File (recommended for ease of use)
Locate the included link to download the BookGenreGenerator.exe file.
Double-click the .exe file to launch the application. It will take a couple of minutes to load initially; this is normal.
The application will open to the main landing page.

	Option 2: Use your preferred IDE (PyCharm recommended)
Install Python version 3.9 or higher here.
Download the BookGenreGenerator repository
Go to the GitHub repository below.
https://github.com/QuaffleUnicorn/BookGenreGenerator
Download the ZIP file version of the project.
Right-click the ZIP file and select the “Extract All” option.
Choose a destination folder for the extracted files
Open the Project in your preferred IDE (this guide will show the process in PyCharm)
Start your IDE.
Create a new project
Import or drag the files from the BookGenreGenerator folder into the IDE
Set up Dependencies within the IDE
Open the built-in terminal within your IDE.
In PyCharm, near the bottom-left, click on the icon called ‘Terminal’.
If you do not have a built-in terminal, you can also use the Windows Command Prompt
Run the following command that will install the necessary libraries:
py -m pip install torch==2.7.1 transformers==4.55.3 scikit-learn==1.6.1 pandas==2.3.2 numpy==2.0.2 nltk==3.9.1 bertopic==0.17.3 matplotlib==3.9.4 seaborn==0.13.2 wordcloud==1.9.4 Pillow==11.0.0
Run Model Training
Within the IDE, locate the folder named RunningModel.py.
Choose the Run option.
The terminal will display when the training and creation of visuals are both complete and automatically stop running. Running this will take at least 20 minutes, if not longer.
Run the Application UI.
Within the IDE, locate the folder named BookGenreGenerator.py.
 While located within the BookGenreGenerator.py file, choose Run.
The application may take several minutes to launch; this is normal.

Getting Started
Upon launching the application, the main landing page will be presented. This page will allow you to click one of three buttons, which will take you to the summary input page, dashboard, or exit the application.

The summary input page includes a text box to input custom summaries, a predict genre button to generate genre predictions, and a back to home button that takes the user back to the main landing page.



Once a summary has been submitted, the application will generate the summary's top 3 genres based on the model’s predictions, along with a confidence score. Additionally, a word cloud will be produced based on the highest-ranked genre, visualizing key terms learned by the model through training.


The dashboard page includes 4 visualizations of the model and the training dataset, a classification report (with precision, recall, F1-score, and support values), a confusion matrix heatmap across all distinct genres, a pie chart showing the distribution of genres across the dataset, and a line graph showing the average loss on the training vs validation datasets.

At the top of the dashboard, there is a dropdown to select a specific type of visualization from the variations mentioned previously. Zooming functionalities are included at the bottom of the screen, along with a button that takes the user back to the main landing page.





Reference Page
California Department of Justice. California Consumer Privacy Act (CCPA). State of California, 13 Mar. 2024, https://oag.ca.gov/privacy/ccpa. Accessed 23 Sept. 2025.
Data Science Process Alliance. "What is CRISP DM?" Data Science PM, https://www.datascience-pm.com/crisp-dm-2/. Accessed 15 Sept. 2025.
GDPR.eu. What Is GDPR? GDPR.eu, 6 Sept. 2018, https://gdpr.eu/what-is-gdpr/. Accessed 23 Sept. 2025.
Hugging Face. DistilBERT — transformers 2.9.1 documentation. Hugging Face, 2020, https://huggingface.co/transformers/v2.9.1/model_doc/distilbert.html. Accessed 15 Sept. 2025.
Ymaricar. CMU Book Summary Dataset. Kaggle, 2018, https://www.kaggle.com/datasets/ymaricar/cmu-book-summary-dataset. Accessed 08 Aug. 2025.
